{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-14T04:05:19.381126300Z",
     "start_time": "2024-08-14T04:04:52.371266Z"
    }
   },
   "source": [
    "from kan import *\n",
    "import torch\n",
    "from torch import nn, optim, autograd\n",
    "from torch import autograd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 定义维度和参数\n",
    "dim = 2\n",
    "np_i = 21  # 内部点数（每个维度）\n",
    "ranges = [-1, 1]\n",
    "\n",
    "# 初始化 KAN 模型\n",
    "model = KAN(width=[2,5,5,1], grid=5, k=3, grid_eps=1.0, noise_scale_base=0.25)\n",
    "\n",
    "def batch_jacobian(func, x, create_graph=False):\n",
    "    # 计算批次雅可比矩阵的函数\n",
    "    def _func_sum(x):\n",
    "        return func(x).sum(dim=0)\n",
    "    return autograd.functional.jacobian(_func_sum, x, create_graph=create_graph).permute(1, 0, 2)\n",
    "\n",
    "# 定义全局参数\n",
    "A = 240  # 最大振幅\n",
    "x0 = 50  # 波形中心位置（网格单位）\n",
    "L = 5    # 波形沿x轴的尺度（网格单位）\n",
    "B = 50   # 潮汐力振幅\n",
    "C = 20   # 风应力系数\n",
    "theta = np.pi / 4  # 风向与x轴的夹角（弧度）\n",
    "D = 30  # 海洋流影响的系数\n",
    "Y_deep = 4000  # 海洋深度（网格单位）\n",
    "\n",
    "# 根据全局参数计算的值\n",
    "kx = 2 * np.pi / L  # x方向的波数\n",
    "ky = 1  # y方向的波数（假设）\n",
    "\n",
    "# 定义解析解函数，只接受空间变量 x\n",
    "sol_fun = lambda x: A * torch.sin(kx * (x[:, 0] - x0)) * torch.cos(ky * x[:, 1])\n",
    "\n",
    "# 定义源项函数，只接受空间变量 x\n",
    "source_fun = lambda x: B * torch.cos(kx * x[:, 0]) * torch.ones_like(x[:, 1]) + C * torch.sin(torch.tensor(theta)) * torch.cos(ky * x[:, 1]) +  D * torch.sin(2 * np.pi * x[:, 1] / Y_deep)\n",
    "\n",
    "# 内部点采样模式 ('mesh' 或 'random')\n",
    "sampling_mode = 'mesh'\n",
    "\n",
    "x_mesh = torch.linspace(ranges[0], ranges[1], steps=np_i)\n",
    "y_mesh = torch.linspace(ranges[0], ranges[1], steps=np_i)\n",
    "X, Y = torch.meshgrid(x_mesh, y_mesh, indexing=\"ij\")\n",
    "\n",
    "if sampling_mode == 'mesh':\n",
    "    # 使用网格点作为内部点\n",
    "    x_i = torch.stack([X.reshape(-1,), Y.reshape(-1,)]).permute(1, 0)\n",
    "else:\n",
    "    # 在域内随机采样内部点\n",
    "    x_i = torch.rand((np_i**2, 2)) * 2 - 1\n",
    "\n",
    "# 边界点（域的四条边）\n",
    "helper = lambda X, Y: torch.stack([X.reshape(-1,), Y.reshape(-1,)]).permute(1, 0)\n",
    "xb1 = helper(X[0], Y[0])\n",
    "xb2 = helper(X[-1], Y[0])\n",
    "xb3 = helper(X[:, 0], Y[:, 0])\n",
    "xb4 = helper(X[:, 0], Y[:, -1])\n",
    "x_b = torch.cat([xb1, xb2, xb3, xb4], dim=0)\n",
    "\n",
    "# 训练参数\n",
    "steps = 10  # 优化步数\n",
    "alpha = 0.5  # PDE 损失相对于边界损失的权重\n",
    "log = 1  # 日志记录频率\n",
    "\n",
    "# 用于存储训练过程中损失的列表\n",
    "pde_losses = []\n",
    "bc_losses = []\n",
    "l2_losses = []\n",
    "losses = []\n",
    "\n",
    "def train():\n",
    "    # 初始化 LBFGS 优化器\n",
    "    optimizer = LBFGS(model.parameters(), lr=0.1, history_size=50, line_search_fn=\"strong_wolfe\", tolerance_grad=1e-32, tolerance_change=1e-32, tolerance_ys=1e-32)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 进度条用于可视化训练进度\n",
    "    pbar = tqdm(range(steps), desc='训练进度')\n",
    "\n",
    "    for _ in pbar:\n",
    "        def closure():\n",
    "            global pde_loss, bc_loss\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 计算内部点的 PDE 损失\n",
    "            sol = sol_fun(x_i)\n",
    "            sol_D1_fun = lambda x: batch_jacobian(model, x, create_graph=True)[:, 0, :]\n",
    "            sol_D1 = sol_D1_fun(x_i)\n",
    "            sol_D2 = batch_jacobian(sol_D1_fun, x_i, create_graph=True)[:, :, :]\n",
    "            lap = torch.sum(torch.diagonal(sol_D2, dim1=1, dim2=2), dim=1, keepdim=True)\n",
    "            source = source_fun(x_i)\n",
    "            # pde_loss = torch.mean((lap - source) ** 2)\n",
    "            delta = 1.0\n",
    "            pde_loss = torch.mean(torch.where(torch.abs(lap - source) < delta, 0.5 * (lap - source)**2, delta * (torch.abs(lap - source) - 0.5 * delta)))\n",
    "\n",
    "            # 计算边界损失\n",
    "            bc_true = sol_fun(x_b)\n",
    "            # bc_true = torch.cat([\n",
    "            #    torch.sin(torch.pi * xb1[:, 1]).unsqueeze(1),\n",
    "            #    torch.sin(torch.pi * xb2[:, 1]).unsqueeze(1),\n",
    "            #    torch.sin(torch.pi * xb3[:, 0]).unsqueeze(1),\n",
    "            #    torch.sin(torch.pi * xb4[:, 0]).unsqueeze(1)\n",
    "            # ], dim=0)\n",
    "            bc_pred = model(x_b)\n",
    "            bc_loss = torch.mean((bc_pred - bc_true) ** 2)\n",
    "\n",
    "            # 总损失\n",
    "            loss = alpha * pde_loss + bc_loss\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        # 执行优化步骤\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        # 计算内部点的 L2 损失\n",
    "        sol = sol_fun(x_i)\n",
    "        l2 = torch.mean((model(x_i) - sol) ** 2)\n",
    "\n",
    "        # 将损失值存入列表\n",
    "        pde_losses.append(pde_loss.item())\n",
    "        bc_losses.append(bc_loss.item())\n",
    "        l2_losses.append(l2.item())\n",
    "        losses.append((alpha * pde_loss + bc_loss).item())\n",
    "\n",
    "        # 更新进度条显示当前损失\n",
    "        if _ % log == 0:\n",
    "            pbar.set_description(f\"PDE 损失: {pde_loss.item():.2e} | 边界损失: {bc_loss.item():.2e} | 总损失: {(alpha * pde_loss + bc_loss).item():.2e}\")\n",
    "\n",
    "    # 返回训练后的模型和损失数据\n",
    "    return model, pde_losses, bc_losses, l2_losses, losses\n",
    "\n",
    "\n",
    "# 开始训练模型并获取结果\n",
    "trained_model, pde_losses, bc_losses, l2_losses, losses = train()\n",
    "\n",
    "# 可视化预测解和解析解\n",
    "with torch.no_grad():\n",
    "    pred_solution = trained_model(x_i).reshape(np_i, np_i).cpu().numpy()\n",
    "    exact_solution = sol_fun(x_i).reshape(np_i, np_i).cpu().numpy()\n",
    "\n",
    "plt.rcParams['font.family'] = ['Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pde_losses, label='PDE 损失')\n",
    "plt.plot(bc_losses, label='边界损失')\n",
    "plt.plot(l2_losses, label='L2 损失')\n",
    "plt.plot(losses, label='总损失')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('损失值')\n",
    "plt.legend()\n",
    "plt.title('训练过程中的损失变化')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDE 损失: 2.36e+01 | 边界损失: 2.18e+04 | 总损失: 2.18e+04:  40%|████      | 4/10 [00:24<00:37,  6.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 138\u001B[0m\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model, pde_losses, bc_losses, l2_losses, losses\n\u001B[0;32m    137\u001B[0m \u001B[38;5;66;03m# 开始训练模型并获取结果\u001B[39;00m\n\u001B[1;32m--> 138\u001B[0m trained_model, pde_losses, bc_losses, l2_losses, losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;66;03m# 可视化预测解和解析解\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "Cell \u001B[1;32mIn[2], line 117\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\n\u001B[0;32m    116\u001B[0m \u001B[38;5;66;03m# 执行优化步骤\u001B[39;00m\n\u001B[1;32m--> 117\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;66;03m# 计算内部点的 L2 损失\u001B[39;00m\n\u001B[0;32m    120\u001B[0m sol \u001B[38;5;241m=\u001B[39m sol_fun(x_i)\n",
      "File \u001B[1;32mE:\\18753\\miniconda\\envs\\Pykan\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    383\u001B[0m             )\n\u001B[1;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mE:\\18753\\miniconda\\envs\\Pykan\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\18753\\软件实践\\18 KAN\\KAN\\源代码\\pykan\\pykan-master\\pykan-master\\kan\\LBFGS.py:433\u001B[0m, in \u001B[0;36mLBFGS.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    430\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobj_func\u001B[39m(x, t, d):\n\u001B[0;32m    431\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_directional_evaluate(closure, x, t, d)\n\u001B[1;32m--> 433\u001B[0m     loss, flat_grad, t, ls_func_evals \u001B[38;5;241m=\u001B[39m \u001B[43m_strong_wolfe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobj_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflat_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgtd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_add_grad(t, d)\n\u001B[0;32m    436\u001B[0m opt_cond \u001B[38;5;241m=\u001B[39m flat_grad\u001B[38;5;241m.\u001B[39mabs()\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m tolerance_grad\n",
      "File \u001B[1;32mE:\\18753\\软件实践\\18 KAN\\KAN\\源代码\\pykan\\pykan-master\\pykan-master\\kan\\LBFGS.py:50\u001B[0m, in \u001B[0;36m_strong_wolfe\u001B[1;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001B[0m\n\u001B[0;32m     48\u001B[0m g \u001B[38;5;241m=\u001B[39m g\u001B[38;5;241m.\u001B[39mclone(memory_format\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mcontiguous_format)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# evaluate objective and gradient using initial step\u001B[39;00m\n\u001B[1;32m---> 50\u001B[0m f_new, g_new \u001B[38;5;241m=\u001B[39m \u001B[43mobj_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m ls_func_evals \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     52\u001B[0m gtd_new \u001B[38;5;241m=\u001B[39m g_new\u001B[38;5;241m.\u001B[39mdot(d)\n",
      "File \u001B[1;32mE:\\18753\\软件实践\\18 KAN\\KAN\\源代码\\pykan\\pykan-master\\pykan-master\\kan\\LBFGS.py:431\u001B[0m, in \u001B[0;36mLBFGS.step.<locals>.obj_func\u001B[1;34m(x, t, d)\u001B[0m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobj_func\u001B[39m(x, t, d):\n\u001B[1;32m--> 431\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_directional_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\18753\\软件实践\\18 KAN\\KAN\\源代码\\pykan\\pykan-master\\pykan-master\\kan\\LBFGS.py:283\u001B[0m, in \u001B[0;36mLBFGS._directional_evaluate\u001B[1;34m(self, closure, x, t, d)\u001B[0m\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_directional_evaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, closure, x, t, d):\n\u001B[0;32m    282\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_add_grad(t, d)\n\u001B[1;32m--> 283\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    284\u001B[0m     flat_grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gather_flat_grad()\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_param(x)\n",
      "File \u001B[1;32mE:\\18753\\miniconda\\envs\\Pykan\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[2], line 93\u001B[0m, in \u001B[0;36mtrain.<locals>.closure\u001B[1;34m()\u001B[0m\n\u001B[0;32m     91\u001B[0m sol_D1_fun \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: batch_jacobian(model, x, create_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[:, \u001B[38;5;241m0\u001B[39m, :]\n\u001B[0;32m     92\u001B[0m sol_D1 \u001B[38;5;241m=\u001B[39m sol_D1_fun(x_i)\n\u001B[1;32m---> 93\u001B[0m sol_D2 \u001B[38;5;241m=\u001B[39m \u001B[43mbatch_jacobian\u001B[49m\u001B[43m(\u001B[49m\u001B[43msol_D1_fun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m[:, :, :]\n\u001B[0;32m     94\u001B[0m lap \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(torch\u001B[38;5;241m.\u001B[39mdiagonal(sol_D2, dim1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, dim2\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     95\u001B[0m source \u001B[38;5;241m=\u001B[39m source_fun(x_i)\n",
      "Cell \u001B[1;32mIn[2], line 21\u001B[0m, in \u001B[0;36mbatch_jacobian\u001B[1;34m(func, x, create_graph)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_func_sum\u001B[39m(x):\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(x)\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjacobian\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_func_sum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_graph\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32mE:\\18753\\miniconda\\envs\\Pykan\\lib\\site-packages\\torch\\autograd\\functional.py:786\u001B[0m, in \u001B[0;36mjacobian\u001B[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001B[0m\n\u001B[0;32m    784\u001B[0m jac_i: Tuple[List[torch\u001B[38;5;241m.\u001B[39mTensor]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m([] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(inputs)))  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m    785\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(out\u001B[38;5;241m.\u001B[39mnelement()):\n\u001B[1;32m--> 786\u001B[0m     vj \u001B[38;5;241m=\u001B[39m \u001B[43m_autograd_grad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    787\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    793\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\n\u001B[0;32m    794\u001B[0m         \u001B[38;5;28mzip\u001B[39m(jac_i, vj, inputs)\n\u001B[0;32m    795\u001B[0m     ):\n\u001B[0;32m    796\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m vj_el \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\18753\\miniconda\\envs\\Pykan\\lib\\site-packages\\torch\\autograd\\functional.py:192\u001B[0m, in \u001B[0;36m_autograd_grad\u001B[1;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001B[0m\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m,) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(inputs)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnew_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnew_grad_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_unused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_grads_batched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\18753\\miniconda\\envs\\Pykan\\lib\\site-packages\\torch\\autograd\\__init__.py:411\u001B[0m, in \u001B[0;36mgrad\u001B[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001B[0m\n\u001B[0;32m    407\u001B[0m     result \u001B[38;5;241m=\u001B[39m _vmap_internals\u001B[38;5;241m.\u001B[39m_vmap(vjp, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, allow_none_pass_through\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)(\n\u001B[0;32m    408\u001B[0m         grad_outputs_\n\u001B[0;32m    409\u001B[0m     )\n\u001B[0;32m    410\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 411\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_outputs_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_unused\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    419\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m materialize_grads:\n\u001B[0;32m    421\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\n\u001B[0;32m    422\u001B[0m         result[i] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tensor_like(inputs[i])\n\u001B[0;32m    423\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(inputs))\n\u001B[0;32m    424\u001B[0m     ):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T04:05:19.387847900Z",
     "start_time": "2024-08-14T04:05:19.381984500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 绘制预测解和解析解\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(pred_solution, extent=[ranges[0], ranges[1], ranges[0], ranges[1]], origin='lower')\n",
    "plt.colorbar(label='预测解')\n",
    "plt.title('预测解')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(exact_solution, extent=[ranges[0], ranges[1], ranges[0], ranges[1]], origin='lower')\n",
    "plt.colorbar(label='解析解')\n",
    "plt.title('解析解')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cbe8ede46bee625c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-14T04:05:19.384847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 绘制损失\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss using LBFGS')\n",
    "plt.show()"
   ],
   "id": "630bb1e346432cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-14T04:05:19.386847200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.plot()"
   ],
   "id": "271a8c3c35356282",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-14T04:05:19.388848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.auto_symbolic()"
   ],
   "id": "cef617fa52e9d913",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-14T04:05:19.389848300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "formula, var = model.symbolic_formula(floating_digit=5)\n",
    "formula[0]"
   ],
   "id": "f72adb7d4ddb7561",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-14T04:05:19.392046500Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "eaf61bee42fb6708",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
